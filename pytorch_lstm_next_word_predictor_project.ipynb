{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishanbhattacharjee12/Next-Word-Predictor-DL-Project/blob/main/pytorch_lstm_next_word_predictor_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NfcpnluzLzo",
        "outputId": "9ac58e6a-3670-42d0-a358-40dcc88bb3d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk"
      ],
      "metadata": {
        "id": "ae7dunqczT8Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document = \"\"\"About Our Voyages\n",
        "What is the travel package for the Martian Colony Initiative (MCI 2025)?\n",
        "The initiative uses a monthly preparation plan where you make payments of $899/month to cover training and logistics.\n",
        "\n",
        "What is the total duration of the pre-flight program?\n",
        "The total duration of the pre-flight training and preparation is 8 months. Therefore, the total preparation fee becomes 899*8 = $7192 (approx.).\n",
        "\n",
        "What is the syllabus of the astronaut preparation program?\n",
        "We will be covering the following modules:\n",
        "\n",
        "Zero-Gravity Acclimatization\n",
        "\n",
        "Exo-geology and Planetary Science\n",
        "\n",
        "Hydroponics and Sustainable Farming\n",
        "\n",
        "SQL for Celestial Body Cataloging\n",
        "\n",
        "Orbital Mechanics and Propulsion Theory\n",
        "\n",
        "Spacesuit Maintenance Algorithms\n",
        "\n",
        "Practical Rover Operation\n",
        "\n",
        "Emergency Protocols\n",
        "\n",
        "Case studies of past missions\n",
        "You can check the detailed syllabus here - https://starlight-voyages.com/courses/Starlight-Martian-Colony-Initiative-848449bfe4b0615a1bbed401\n",
        "\n",
        "Will training for the Outer Planets be a part of this program?\n",
        "No, Jupiter and Saturn missions are not a part of this program’s curriculum. This is strictly focused on Mars.\n",
        "\n",
        "What if I miss a live simulation? Will I get a recording of it?\n",
        "Yes, all our theoretical sessions are recorded, so even if you miss a session you can go back and watch the recording. Practical simulations must be rescheduled.\n",
        "\n",
        "Where can I find the training schedule?\n",
        "Check out this data sheet to see the month-by-month timetable of the program - https://docs.google.com/spreadsheets/d/27PpTax_B7ORBeDg4emgexhqqPv3noQPYKU7RJ6ArOzk/edit?usp=sharing.\n",
        "\n",
        "What is the time duration of all the live training sessions?\n",
        "Roughly, all sessions last 3 hours.\n",
        "\n",
        "What is the language spoken by the commander during the sessions?\n",
        "English.\n",
        "\n",
        "How will I be informed about the upcoming training module?\n",
        "You will get a priority communication from our side before every module once you become a registered candidate.\n",
        "\n",
        "Can I do this program if I am from a non-science background?\n",
        "Yes, absolutely.\n",
        "\n",
        "I am late, can I join the program in the middle?\n",
        "Absolutely, you can join the program anytime.\n",
        "\n",
        "If I join/pay in the middle, will I be able to see all the past simulations?\n",
        "Yes, once you make the payment you will be able to see all the past theoretical content in your flight dashboard.\n",
        "\n",
        "Where do I have to submit my completed tasks?\n",
        "You don’t have to submit the tasks. We will provide you with the solution parameters, you have to self-evaluate your performance.\n",
        "\n",
        "Will we do mission case studies in the program?\n",
        "Yes.\n",
        "\n",
        "Where can we contact you?\n",
        "You can send a transmission to mission.control@starlight-voyages.com.\n",
        "\n",
        "Payment/Registration Related Questions\n",
        "Where do we have to make our payments? On a partner site or the main website?\n",
        "You have to make all your monthly payments on our official website. Here is the link for our website - https://starlight-voyages.com/\n",
        "\n",
        "Can we pay the entire amount of $7192 all at once?\n",
        "Unfortunately no, the program follows a monthly preparation model.\n",
        "\n",
        "What is the validity of a monthly subscription? Suppose if I pay on June 15th, then do I have to pay again on July 1st or July 15th?\n",
        "July 15th. The validity period is 30 days from the day you make the payment. You can join anytime without waiting for a month to end.\n",
        "\n",
        "What if I don’t like the training after making the payment. What is the refund policy?\n",
        "You get a 7-day refund period from the day you have made the payment.\n",
        "\n",
        "I am applying from a different country and I am not able to make the payment on the website, what should I do?\n",
        "You have to contact us by sending a transmission to mission.control@starlight-voyages.com.\n",
        "\n",
        "Post-Registration Queries\n",
        "Till when can I view the paid training videos on the website?\n",
        "This one is tricky, so read carefully. You can watch the videos till your subscription is valid. Suppose you have purchased a subscription on April 21st, you will be able to watch all the past paid sessions in the period of April 21st to May 20th. But after May 21st, you will have to purchase the subscription again. Once the program is over and you have paid us $7192 (or 8 installments of $899) you will be able to watch the paid sessions until the launch window in 2026 closes.\n",
        "\n",
        "Why is lifetime validity not provided?\n",
        "Because mission parameters and technology are constantly evolving.\n",
        "\n",
        "Where can I reach out in case of a doubt after a session?\n",
        "You will have to fill a communication request form provided in your dashboard and our team will contact you for a 1-on-1 debriefing session.\n",
        "\n",
        "If I join the program late, can I still ask about past modules?\n",
        "Yes, just select the past module topic in the communication request form.\n",
        "\n",
        "Certificate and Mission Placement Related Queries\n",
        "What is the criteria to get the certificate of completion?\n",
        "There are 2 criteria: You have to pay the entire fee of $7192, and you have to pass all the mission readiness assessments.\n",
        "\n",
        "I am joining late. How can I pay the fee of the earlier months?\n",
        "You will get a link to pay the fee for earlier months in your dashboard once you pay for the current month.\n",
        "\n",
        "I have read that Mission Placement is a part of this program. What comes under Mission Placement?\n",
        "This is to clarify that program completion does not mean a guarantee of a mission seat. We do not guarantee you a job or a spot on the Ares-IV crew. If you are joining this course just for a guaranteed flight, you will be disappointed. Here is what comes under mission placement consideration:\n",
        "\n",
        "Advanced Portfolio Building\n",
        "\n",
        "Psychological skills sessions\n",
        "\n",
        "Sessions with veteran astronauts\n",
        "\n",
        "Discussion on mission selection strategies\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "fLwm0Y_MzVuh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMU_RwfbzXt4",
        "outputId": "34e89042-9c8f-4fb0-a86b-14cf823141a7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize\n",
        "tokens = word_tokenize(document.lower())"
      ],
      "metadata": {
        "id": "t28bgAcszaHl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build vocab\n",
        "vocab = {'<unk>':0}\n",
        "\n",
        "for token in Counter(tokens).keys():\n",
        "  if token not in vocab:\n",
        "    vocab[token] = len(vocab)\n",
        "\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G30GxEjgzcfY",
        "outputId": "6fe515a2-a914-4612-adfe-b8c96a0f7cba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<unk>': 0,\n",
              " 'about': 1,\n",
              " 'our': 2,\n",
              " 'voyages': 3,\n",
              " 'what': 4,\n",
              " 'is': 5,\n",
              " 'the': 6,\n",
              " 'travel': 7,\n",
              " 'package': 8,\n",
              " 'for': 9,\n",
              " 'martian': 10,\n",
              " 'colony': 11,\n",
              " 'initiative': 12,\n",
              " '(': 13,\n",
              " 'mci': 14,\n",
              " '2025': 15,\n",
              " ')': 16,\n",
              " '?': 17,\n",
              " 'uses': 18,\n",
              " 'a': 19,\n",
              " 'monthly': 20,\n",
              " 'preparation': 21,\n",
              " 'plan': 22,\n",
              " 'where': 23,\n",
              " 'you': 24,\n",
              " 'make': 25,\n",
              " 'payments': 26,\n",
              " 'of': 27,\n",
              " '$': 28,\n",
              " '899/month': 29,\n",
              " 'to': 30,\n",
              " 'cover': 31,\n",
              " 'training': 32,\n",
              " 'and': 33,\n",
              " 'logistics': 34,\n",
              " '.': 35,\n",
              " 'total': 36,\n",
              " 'duration': 37,\n",
              " 'pre-flight': 38,\n",
              " 'program': 39,\n",
              " '8': 40,\n",
              " 'months': 41,\n",
              " 'therefore': 42,\n",
              " ',': 43,\n",
              " 'fee': 44,\n",
              " 'becomes': 45,\n",
              " '899': 46,\n",
              " '*': 47,\n",
              " '=': 48,\n",
              " '7192': 49,\n",
              " 'approx.': 50,\n",
              " 'syllabus': 51,\n",
              " 'astronaut': 52,\n",
              " 'we': 53,\n",
              " 'will': 54,\n",
              " 'be': 55,\n",
              " 'covering': 56,\n",
              " 'following': 57,\n",
              " 'modules': 58,\n",
              " ':': 59,\n",
              " 'zero-gravity': 60,\n",
              " 'acclimatization': 61,\n",
              " 'exo-geology': 62,\n",
              " 'planetary': 63,\n",
              " 'science': 64,\n",
              " 'hydroponics': 65,\n",
              " 'sustainable': 66,\n",
              " 'farming': 67,\n",
              " 'sql': 68,\n",
              " 'celestial': 69,\n",
              " 'body': 70,\n",
              " 'cataloging': 71,\n",
              " 'orbital': 72,\n",
              " 'mechanics': 73,\n",
              " 'propulsion': 74,\n",
              " 'theory': 75,\n",
              " 'spacesuit': 76,\n",
              " 'maintenance': 77,\n",
              " 'algorithms': 78,\n",
              " 'practical': 79,\n",
              " 'rover': 80,\n",
              " 'operation': 81,\n",
              " 'emergency': 82,\n",
              " 'protocols': 83,\n",
              " 'case': 84,\n",
              " 'studies': 85,\n",
              " 'past': 86,\n",
              " 'missions': 87,\n",
              " 'can': 88,\n",
              " 'check': 89,\n",
              " 'detailed': 90,\n",
              " 'here': 91,\n",
              " '-': 92,\n",
              " 'https': 93,\n",
              " '//starlight-voyages.com/courses/starlight-martian-colony-initiative-848449bfe4b0615a1bbed401': 94,\n",
              " 'outer': 95,\n",
              " 'planets': 96,\n",
              " 'part': 97,\n",
              " 'this': 98,\n",
              " 'no': 99,\n",
              " 'jupiter': 100,\n",
              " 'saturn': 101,\n",
              " 'are': 102,\n",
              " 'not': 103,\n",
              " '’': 104,\n",
              " 's': 105,\n",
              " 'curriculum': 106,\n",
              " 'strictly': 107,\n",
              " 'focused': 108,\n",
              " 'on': 109,\n",
              " 'mars': 110,\n",
              " 'if': 111,\n",
              " 'i': 112,\n",
              " 'miss': 113,\n",
              " 'live': 114,\n",
              " 'simulation': 115,\n",
              " 'get': 116,\n",
              " 'recording': 117,\n",
              " 'it': 118,\n",
              " 'yes': 119,\n",
              " 'all': 120,\n",
              " 'theoretical': 121,\n",
              " 'sessions': 122,\n",
              " 'recorded': 123,\n",
              " 'so': 124,\n",
              " 'even': 125,\n",
              " 'session': 126,\n",
              " 'go': 127,\n",
              " 'back': 128,\n",
              " 'watch': 129,\n",
              " 'simulations': 130,\n",
              " 'must': 131,\n",
              " 'rescheduled': 132,\n",
              " 'find': 133,\n",
              " 'schedule': 134,\n",
              " 'out': 135,\n",
              " 'data': 136,\n",
              " 'sheet': 137,\n",
              " 'see': 138,\n",
              " 'month-by-month': 139,\n",
              " 'timetable': 140,\n",
              " '//docs.google.com/spreadsheets/d/27pptax_b7orbedg4emgexhqqpv3noqpyku7rj6arozk/edit': 141,\n",
              " 'usp=sharing': 142,\n",
              " 'time': 143,\n",
              " 'roughly': 144,\n",
              " 'last': 145,\n",
              " '3': 146,\n",
              " 'hours': 147,\n",
              " 'language': 148,\n",
              " 'spoken': 149,\n",
              " 'by': 150,\n",
              " 'commander': 151,\n",
              " 'during': 152,\n",
              " 'english': 153,\n",
              " 'how': 154,\n",
              " 'informed': 155,\n",
              " 'upcoming': 156,\n",
              " 'module': 157,\n",
              " 'priority': 158,\n",
              " 'communication': 159,\n",
              " 'from': 160,\n",
              " 'side': 161,\n",
              " 'before': 162,\n",
              " 'every': 163,\n",
              " 'once': 164,\n",
              " 'become': 165,\n",
              " 'registered': 166,\n",
              " 'candidate': 167,\n",
              " 'do': 168,\n",
              " 'am': 169,\n",
              " 'non-science': 170,\n",
              " 'background': 171,\n",
              " 'absolutely': 172,\n",
              " 'late': 173,\n",
              " 'join': 174,\n",
              " 'in': 175,\n",
              " 'middle': 176,\n",
              " 'anytime': 177,\n",
              " 'join/pay': 178,\n",
              " 'able': 179,\n",
              " 'payment': 180,\n",
              " 'content': 181,\n",
              " 'your': 182,\n",
              " 'flight': 183,\n",
              " 'dashboard': 184,\n",
              " 'have': 185,\n",
              " 'submit': 186,\n",
              " 'my': 187,\n",
              " 'completed': 188,\n",
              " 'tasks': 189,\n",
              " 'don': 190,\n",
              " 't': 191,\n",
              " 'provide': 192,\n",
              " 'with': 193,\n",
              " 'solution': 194,\n",
              " 'parameters': 195,\n",
              " 'self-evaluate': 196,\n",
              " 'performance': 197,\n",
              " 'mission': 198,\n",
              " 'contact': 199,\n",
              " 'send': 200,\n",
              " 'transmission': 201,\n",
              " 'mission.control': 202,\n",
              " '@': 203,\n",
              " 'starlight-voyages.com': 204,\n",
              " 'payment/registration': 205,\n",
              " 'related': 206,\n",
              " 'questions': 207,\n",
              " 'partner': 208,\n",
              " 'site': 209,\n",
              " 'or': 210,\n",
              " 'main': 211,\n",
              " 'website': 212,\n",
              " 'official': 213,\n",
              " 'link': 214,\n",
              " '//starlight-voyages.com/': 215,\n",
              " 'pay': 216,\n",
              " 'entire': 217,\n",
              " 'amount': 218,\n",
              " 'at': 219,\n",
              " 'unfortunately': 220,\n",
              " 'follows': 221,\n",
              " 'model': 222,\n",
              " 'validity': 223,\n",
              " 'subscription': 224,\n",
              " 'suppose': 225,\n",
              " 'june': 226,\n",
              " '15th': 227,\n",
              " 'then': 228,\n",
              " 'again': 229,\n",
              " 'july': 230,\n",
              " '1st': 231,\n",
              " 'period': 232,\n",
              " '30': 233,\n",
              " 'days': 234,\n",
              " 'day': 235,\n",
              " 'without': 236,\n",
              " 'waiting': 237,\n",
              " 'month': 238,\n",
              " 'end': 239,\n",
              " 'like': 240,\n",
              " 'after': 241,\n",
              " 'making': 242,\n",
              " 'refund': 243,\n",
              " 'policy': 244,\n",
              " '7-day': 245,\n",
              " 'made': 246,\n",
              " 'applying': 247,\n",
              " 'different': 248,\n",
              " 'country': 249,\n",
              " 'should': 250,\n",
              " 'us': 251,\n",
              " 'sending': 252,\n",
              " 'post-registration': 253,\n",
              " 'queries': 254,\n",
              " 'till': 255,\n",
              " 'when': 256,\n",
              " 'view': 257,\n",
              " 'paid': 258,\n",
              " 'videos': 259,\n",
              " 'one': 260,\n",
              " 'tricky': 261,\n",
              " 'read': 262,\n",
              " 'carefully': 263,\n",
              " 'valid': 264,\n",
              " 'purchased': 265,\n",
              " 'april': 266,\n",
              " '21st': 267,\n",
              " 'may': 268,\n",
              " '20th': 269,\n",
              " 'but': 270,\n",
              " 'purchase': 271,\n",
              " 'over': 272,\n",
              " 'installments': 273,\n",
              " 'until': 274,\n",
              " 'launch': 275,\n",
              " 'window': 276,\n",
              " '2026': 277,\n",
              " 'closes': 278,\n",
              " 'why': 279,\n",
              " 'lifetime': 280,\n",
              " 'provided': 281,\n",
              " 'because': 282,\n",
              " 'technology': 283,\n",
              " 'constantly': 284,\n",
              " 'evolving': 285,\n",
              " 'reach': 286,\n",
              " 'doubt': 287,\n",
              " 'fill': 288,\n",
              " 'request': 289,\n",
              " 'form': 290,\n",
              " 'team': 291,\n",
              " '1-on-1': 292,\n",
              " 'debriefing': 293,\n",
              " 'still': 294,\n",
              " 'ask': 295,\n",
              " 'just': 296,\n",
              " 'select': 297,\n",
              " 'topic': 298,\n",
              " 'certificate': 299,\n",
              " 'placement': 300,\n",
              " 'criteria': 301,\n",
              " 'completion': 302,\n",
              " 'there': 303,\n",
              " '2': 304,\n",
              " 'pass': 305,\n",
              " 'readiness': 306,\n",
              " 'assessments': 307,\n",
              " 'joining': 308,\n",
              " 'earlier': 309,\n",
              " 'current': 310,\n",
              " 'that': 311,\n",
              " 'comes': 312,\n",
              " 'under': 313,\n",
              " 'clarify': 314,\n",
              " 'does': 315,\n",
              " 'mean': 316,\n",
              " 'guarantee': 317,\n",
              " 'seat': 318,\n",
              " 'job': 319,\n",
              " 'spot': 320,\n",
              " 'ares-iv': 321,\n",
              " 'crew': 322,\n",
              " 'course': 323,\n",
              " 'guaranteed': 324,\n",
              " 'disappointed': 325,\n",
              " 'consideration': 326,\n",
              " 'advanced': 327,\n",
              " 'portfolio': 328,\n",
              " 'building': 329,\n",
              " 'psychological': 330,\n",
              " 'skills': 331,\n",
              " 'veteran': 332,\n",
              " 'astronauts': 333,\n",
              " 'discussion': 334,\n",
              " 'selection': 335,\n",
              " 'strategies': 336}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOOEZ94P0dQ1",
        "outputId": "b6663768-64f3-4857-8ae1-ed3c06f1efd7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "337"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentences = document.split('\\n')"
      ],
      "metadata": {
        "id": "RefNavJe1Cva"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_indices(sentence, vocab):\n",
        "\n",
        "  numerical_sentence = []\n",
        "\n",
        "  for token in sentence:\n",
        "    if token in vocab:\n",
        "      numerical_sentence.append(vocab[token])\n",
        "    else:\n",
        "      numerical_sentence.append(vocab['<unk>'])\n",
        "\n",
        "  return numerical_sentence\n"
      ],
      "metadata": {
        "id": "x52A3E1K1zjn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_numerical_sentences = []\n",
        "\n",
        "for sentence in input_sentences:\n",
        "  input_numerical_sentences.append(text_to_indices(word_tokenize(sentence.lower()), vocab))\n"
      ],
      "metadata": {
        "id": "eu66Zo3e1Wh9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(input_numerical_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxJesAQC1et3",
        "outputId": "9a15d980-71da-4539-9161-6445d3c191ee"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "112"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_sequence = []\n",
        "for sentence in input_numerical_sentences:\n",
        "\n",
        "  for i in range(1, len(sentence)):\n",
        "    training_sequence.append(sentence[:i+1])"
      ],
      "metadata": {
        "id": "80rIx4aq6ele"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(training_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_aGJ0fy7swk",
        "outputId": "607e4160-9b97-4b9b-8593-bc53ff64f6cb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "968"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_sequence[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrFzZ4DD8Anu",
        "outputId": "fcdd427c-4b68-4788-9ecd-e30fa2ac0425"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2], [1, 2, 3], [4, 5], [4, 5, 6], [4, 5, 6, 7]]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_list = []\n",
        "\n",
        "for sequence in training_sequence:\n",
        "  len_list.append(len(sequence))\n",
        "\n",
        "max(len_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2Z_fiVZ8GRo",
        "outputId": "4ace307c-9557-4d7f-a4c2-83e3403fdaf7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "102"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_sequence[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bIcIRd088EN",
        "outputId": "022af832-6c7f-482d-ad7a-74397144bc42"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_training_sequence = []\n",
        "for sequence in training_sequence:\n",
        "\n",
        "  padded_training_sequence.append([0]*(max(len_list) - len(sequence)) + sequence)"
      ],
      "metadata": {
        "id": "dtPg5uRN9Cc7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(padded_training_sequence[10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqZssF989X-4",
        "outputId": "0242d746-fa06-4791-c2de-23fc971d607c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "102"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_training_sequence = torch.tensor(padded_training_sequence, dtype=torch.long)"
      ],
      "metadata": {
        "id": "0_wVpepb9iE4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_training_sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogKvdXa79yxV",
        "outputId": "1b3c44ec-9b8e-4323-e51f-25af8a36df93"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0,   0,   0,  ...,   0,   1,   2],\n",
              "        [  0,   0,   0,  ...,   1,   2,   3],\n",
              "        [  0,   0,   0,  ...,   0,   4,   5],\n",
              "        ...,\n",
              "        [  0,   0,   0,  ..., 334, 109, 198],\n",
              "        [  0,   0,   0,  ..., 109, 198, 335],\n",
              "        [  0,   0,   0,  ..., 198, 335, 336]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = padded_training_sequence[:, :-1]\n",
        "y = padded_training_sequence[:,-1]"
      ],
      "metadata": {
        "id": "Tz8fwCok90m0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ed_PLHJ-Dgv",
        "outputId": "f86135f3-f7dc-4e79-cce5-364b5133454f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0,   0,   0,  ...,   0,   0,   1],\n",
              "        [  0,   0,   0,  ...,   0,   1,   2],\n",
              "        [  0,   0,   0,  ...,   0,   0,   4],\n",
              "        ...,\n",
              "        [  0,   0,   0,  ...,   0, 334, 109],\n",
              "        [  0,   0,   0,  ..., 334, 109, 198],\n",
              "        [  0,   0,   0,  ..., 109, 198, 335]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eReVrcX9-EUU",
        "outputId": "4f6b66d2-f382-4cf5-fdd3-982d70f3baf1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  2,   3,   5,   6,   7,   8,   9,   6,  10,  11,  12,  13,  14,  15,\n",
              "         16,  17,  12,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
              "         29,  30,  31,  32,  33,  34,  35,   5,   6,  36,  37,  27,   6,  38,\n",
              "         39,  17,  36,  37,  27,   6,  38,  32,  33,  21,   5,  40,  41,  35,\n",
              "         42,  43,   6,  36,  21,  44,  45,  46,  47,  40,  48,  28,  49,  13,\n",
              "          0,  35,  16,  35,   5,   6,  51,  27,   6,  52,  21,  39,  17,  54,\n",
              "         55,  56,   6,  57,  58,  59,  61,  33,  63,  64,  33,  66,  67,   9,\n",
              "         69,  70,  71,  73,  33,  74,  75,  77,  78,  80,  81,  83,  85,  27,\n",
              "         86,  87,  88,  89,   6,  90,  51,  91,  92,  93,  59,  94,  32,   9,\n",
              "          6,  95,  96,  55,  19,  97,  27,  98,  39,  17,  43, 100,  33, 101,\n",
              "         87, 102, 103,  19,  97,  27,  98,  39, 104, 105, 106,  35,  98,   5,\n",
              "        107, 108, 109, 110,  35, 111, 112, 113,  19, 114, 115,  17,  54, 112,\n",
              "        116,  19, 117,  27, 118,  17,  43, 120,   2, 121, 122, 102, 123,  43,\n",
              "        124, 125, 111,  24, 113,  19, 126,  24,  88, 127, 128,  33, 129,   6,\n",
              "        117,  35,  79, 130, 131,  55, 132,  35,  88, 112, 133,   6,  32, 134,\n",
              "         17, 135,  98, 136, 137,  30, 138,   6, 139, 140,  27,   6,  39,  92,\n",
              "         93,  59, 141,  17, 142,  35,   5,   6, 143,  37,  27, 120,   6, 114,\n",
              "         32, 122,  17,  43, 120, 122, 145, 146, 147,  35,   5,   6, 148, 149,\n",
              "        150,   6, 151, 152,   6, 122,  17,  35,  54, 112,  55, 155,   1,   6,\n",
              "        156,  32, 157,  17,  54, 116,  19, 158, 159, 160,   2, 161, 162, 163,\n",
              "        157, 164,  24, 165,  19, 166, 167,  35, 112, 168,  98,  39, 111, 112,\n",
              "        169, 160,  19, 170, 171,  17,  43, 172,  35, 169, 173,  43,  88, 112,\n",
              "        174,   6,  39, 175,   6, 176,  17,  43,  24,  88, 174,   6,  39, 177,\n",
              "         35, 112, 178, 175,   6, 176,  43,  54, 112,  55, 179,  30, 138, 120,\n",
              "          6,  86, 130,  17,  43, 164,  24,  25,   6, 180,  24,  54,  55, 179,\n",
              "         30, 138, 120,   6,  86, 121, 181, 175, 182, 183, 184,  35, 168, 112,\n",
              "        185,  30, 186, 187, 188, 189,  17, 190, 104, 191, 185,  30, 186,   6,\n",
              "        189,  35,  53,  54, 192,  24, 193,   6, 194, 195,  43,  24, 185,  30,\n",
              "        196, 182, 197,  35,  53, 168, 198,  84,  85, 175,   6,  39,  17,  35,\n",
              "         88,  53, 199,  24,  17,  88, 200,  19, 201,  30, 202, 203, 204,  35,\n",
              "        206, 207, 168,  53, 185,  30,  25,   2,  26,  17, 109,  19, 208, 209,\n",
              "        210,   6, 211, 212,  17, 185,  30,  25, 120, 182,  20,  26, 109,   2,\n",
              "        213, 212,  35,  91,   5,   6, 214,   9,   2, 212,  92,  93,  59, 215,\n",
              "         53, 216,   6, 217, 218,  27,  28,  49, 120, 219, 164,  17,  99,  43,\n",
              "          6,  39, 221,  19,  20,  21, 222,  35,   5,   6, 223,  27,  19,  20,\n",
              "        224,  17, 225, 111, 112, 216, 109, 226, 227,  43, 228, 168, 112, 185,\n",
              "         30, 216, 229, 109, 230, 231, 210, 230, 227,  17, 227,  35,   6, 223,\n",
              "        232,   5, 233, 234, 160,   6, 235,  24,  25,   6, 180,  35,  24,  88,\n",
              "        174, 177, 236, 237,   9,  19, 238,  30, 239,  35, 111, 112, 190, 104,\n",
              "        191, 240,   6,  32, 241, 242,   6, 180,  35,   4,   5,   6, 243, 244,\n",
              "         17, 116,  19, 245, 243, 232, 160,   6, 235,  24, 185, 246,   6, 180,\n",
              "         35, 169, 247, 160,  19, 248, 249,  33, 112, 169, 103, 179,  30,  25,\n",
              "          6, 180, 109,   6, 212,  43,   4, 250, 112, 168,  17, 185,  30, 199,\n",
              "        251, 150, 252,  19, 201,  30, 202, 203, 204,  35, 254, 256,  88, 112,\n",
              "        257,   6, 258,  32, 259, 109,   6, 212,  17, 260,   5, 261,  43, 124,\n",
              "        262, 263,  35,  24,  88, 129,   6, 259, 255, 182, 224,   5, 264,  35,\n",
              "        225,  24, 185, 265,  19, 224, 109, 266, 267,  43,  24,  54,  55, 179,\n",
              "         30, 129, 120,   6,  86, 258, 122, 175,   6, 232,  27, 266, 267,  30,\n",
              "        268, 269,  35, 270, 241, 268, 267,  43,  24,  54, 185,  30, 271,   6,\n",
              "        224, 229,  35, 164,   6,  39,   5, 272,  33,  24, 185, 258, 251,  28,\n",
              "         49,  13, 210,  40, 273,  27,  28,  46,  16,  24,  54,  55, 179,  30,\n",
              "        129,   6, 258, 122, 274,   6, 275, 276, 175, 277, 278,  35,   5, 280,\n",
              "        223, 103, 281,  17, 198, 195,  33, 283, 102, 284, 285,  35,  88, 112,\n",
              "        286, 135, 175,  84,  27,  19, 287, 241,  19, 126,  17,  54, 185,  30,\n",
              "        288,  19, 159, 289, 290, 281, 175, 182, 184,  33,   2, 291,  54, 199,\n",
              "         24,   9,  19, 292, 293, 126,  35, 112, 174,   6,  39, 173,  43,  88,\n",
              "        112, 294, 295,   1,  86,  58,  17,  43, 296, 297,   6,  86, 157, 298,\n",
              "        175,   6, 159, 289, 290,  35,  33, 198, 300, 206, 254,   5,   6, 301,\n",
              "         30, 116,   6, 299,  27, 302,  17, 102, 304, 301,  59,  24, 185,  30,\n",
              "        216,   6, 217,  44,  27,  28,  49,  43,  33,  24, 185,  30, 305, 120,\n",
              "          6, 198, 306, 307,  35, 169, 308, 173,  35, 154,  88, 112, 216,   6,\n",
              "         44,  27,   6, 309,  41,  17,  54, 116,  19, 214,  30, 216,   6,  44,\n",
              "          9, 309,  41, 175, 182, 184, 164,  24, 216,   9,   6, 310, 238,  35,\n",
              "        185, 262, 311, 198, 300,   5,  19,  97,  27,  98,  39,  35,   4, 312,\n",
              "        313, 198, 300,  17,   5,  30, 314, 311,  39, 302, 315, 103, 316,  19,\n",
              "        317,  27,  19, 198, 318,  35,  53, 168, 103, 317,  24,  19, 319, 210,\n",
              "         19, 320, 109,   6, 321, 322,  35, 111,  24, 102, 308,  98, 323, 296,\n",
              "          9,  19, 324, 183,  43,  24,  54,  55, 325,  35,  91,   5,   4, 312,\n",
              "        313, 198, 300, 326,  59, 328, 329, 331, 122, 193, 332, 333, 109, 198,\n",
              "        335, 336])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.X.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "fR059hVd-IAf"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CustomDataset(X,y)"
      ],
      "metadata": {
        "id": "KLX0clQM_j9r"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYHaeSuI_nJX",
        "outputId": "059ae7a9-811a-49b8-85a8-404a80a02ce2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "968"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "7ZUeD3l6_oZ3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, 100)\n",
        "    self.lstm = nn.LSTM(100, 150, batch_first=True)\n",
        "    self.fc = nn.Linear(150, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    embedded = self.embedding(x)\n",
        "    intermediate_hidden_states, (final_hidden_state, final_cell_state) = self.lstm(embedded)\n",
        "    output = self.fc(final_hidden_state.squeeze(0))\n",
        "    return output"
      ],
      "metadata": {
        "id": "0TEukXmWDEn8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMModel(len(vocab))"
      ],
      "metadata": {
        "id": "YcQEVc9aVgr5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "Lvm7W6L1X6P1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXwq43NRYD3q",
        "outputId": "54c5c8e2-2f4a-4a88-dc78-515a8f0c8e6f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (embedding): Embedding(337, 100)\n",
              "  (lstm): LSTM(100, 150, batch_first=True)\n",
              "  (fc): Linear(in_features=150, out_features=337, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "learning_rate = 0.001\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "1faORN1VYFdu"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  total_loss = 0\n",
        "\n",
        "  for batch_x, batch_y in dataloader:\n",
        "\n",
        "    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(batch_x)\n",
        "\n",
        "    loss = criterion(output, batch_y)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "  print(f\"Epoch: {epoch + 1}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRLc1cbrYVVV",
        "outputId": "15e0b19d-35d2-46e1-8244-a64ed90641b5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 176.3735\n",
            "Epoch: 2, Loss: 155.8588\n",
            "Epoch: 3, Loss: 143.8906\n",
            "Epoch: 4, Loss: 132.4578\n",
            "Epoch: 5, Loss: 121.9206\n",
            "Epoch: 6, Loss: 109.8117\n",
            "Epoch: 7, Loss: 100.0251\n",
            "Epoch: 8, Loss: 91.1521\n",
            "Epoch: 9, Loss: 80.8100\n",
            "Epoch: 10, Loss: 73.1943\n",
            "Epoch: 11, Loss: 65.6868\n",
            "Epoch: 12, Loss: 59.1512\n",
            "Epoch: 13, Loss: 52.2855\n",
            "Epoch: 14, Loss: 46.1781\n",
            "Epoch: 15, Loss: 41.6570\n",
            "Epoch: 16, Loss: 36.3985\n",
            "Epoch: 17, Loss: 32.2755\n",
            "Epoch: 18, Loss: 29.2108\n",
            "Epoch: 19, Loss: 25.8524\n",
            "Epoch: 20, Loss: 22.9233\n",
            "Epoch: 21, Loss: 20.4179\n",
            "Epoch: 22, Loss: 18.2075\n",
            "Epoch: 23, Loss: 16.6323\n",
            "Epoch: 24, Loss: 15.1114\n",
            "Epoch: 25, Loss: 13.7260\n",
            "Epoch: 26, Loss: 12.5789\n",
            "Epoch: 27, Loss: 11.6320\n",
            "Epoch: 28, Loss: 10.7166\n",
            "Epoch: 29, Loss: 10.1173\n",
            "Epoch: 30, Loss: 9.3051\n",
            "Epoch: 31, Loss: 8.6921\n",
            "Epoch: 32, Loss: 8.1363\n",
            "Epoch: 33, Loss: 8.1175\n",
            "Epoch: 34, Loss: 7.4857\n",
            "Epoch: 35, Loss: 7.1149\n",
            "Epoch: 36, Loss: 6.6029\n",
            "Epoch: 37, Loss: 6.4809\n",
            "Epoch: 38, Loss: 6.0700\n",
            "Epoch: 39, Loss: 5.8745\n",
            "Epoch: 40, Loss: 5.7473\n",
            "Epoch: 41, Loss: 5.5056\n",
            "Epoch: 42, Loss: 5.4789\n",
            "Epoch: 43, Loss: 5.1069\n",
            "Epoch: 44, Loss: 4.9892\n",
            "Epoch: 45, Loss: 5.0904\n",
            "Epoch: 46, Loss: 4.7878\n",
            "Epoch: 47, Loss: 4.5844\n",
            "Epoch: 48, Loss: 4.4303\n",
            "Epoch: 49, Loss: 4.3881\n",
            "Epoch: 50, Loss: 4.7692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction\n",
        "\n",
        "def prediction(model, vocab, text):\n",
        "\n",
        "  # tokenize\n",
        "  tokenized_text = word_tokenize(text.lower())\n",
        "\n",
        "  # text -> numerical indices\n",
        "  numerical_text = text_to_indices(tokenized_text, vocab)\n",
        "\n",
        "  # padding\n",
        "  padded_text = torch.tensor([0] * (61 - len(numerical_text)) + numerical_text, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "  # send to model\n",
        "  output = model(padded_text)\n",
        "\n",
        "  # predicted index\n",
        "  value, index = torch.max(output, dim=1)\n",
        "\n",
        "  # merge with text\n",
        "  return text + \" \" + list(vocab.keys())[index]\n",
        "\n"
      ],
      "metadata": {
        "id": "O9f6DkX-ZM-r"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "num_tokens = 10\n",
        "input_text = \"Why is lifetime validity not provided?\"\n",
        "\n",
        "for i in range(num_tokens):\n",
        "  output_text = prediction(model, vocab, input_text)\n",
        "  print(output_text)\n",
        "  input_text = output_text\n",
        "  time.sleep(0.5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_JPACfEbNPo",
        "outputId": "60ed8960-927d-456a-aad8-911f1b849caa"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why is lifetime validity not provided? ?\n",
            "Why is lifetime validity not provided? ? on\n",
            "Why is lifetime validity not provided? ? on a\n",
            "Why is lifetime validity not provided? ? on a partner\n",
            "Why is lifetime validity not provided? ? on a partner site\n",
            "Why is lifetime validity not provided? ? on a partner site or\n",
            "Why is lifetime validity not provided? ? on a partner site or the\n",
            "Why is lifetime validity not provided? ? on a partner site or the main\n",
            "Why is lifetime validity not provided? ? on a partner site or the main website\n",
            "Why is lifetime validity not provided? ? on a partner site or the main website ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader1 = DataLoader(dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "JXsV4AnNXNnw"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(model, dataloader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # No need to compute gradients\n",
        "        for batch_x, batch_y in dataloader1:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "            # Get model predictions\n",
        "            outputs = model(batch_x)\n",
        "\n",
        "            # Get the predicted word indices\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "\n",
        "            # Compare with actual labels\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "            total += batch_y.size(0)\n",
        "\n",
        "    accuracy = correct / total * 100\n",
        "    return accuracy\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = calculate_accuracy(model, dataloader, device)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "Py7o0rJJc5pm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a19fac5-5caa-4cce-fcaa-964d3fb6fbea"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 96.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0bQnBuShXG5i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}